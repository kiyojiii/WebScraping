{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af49d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Name               Start Time  \\\n",
      "0    Bulusan     2022 Jun 12 03:37 AM   \n",
      "1    Bulusan     2022 Jun 05 10:37 AM   \n",
      "2       Taal  2021 Jul 01-2022 Mar 27   \n",
      "3       Taal      2020 Jan 12 1:00 PM   \n",
      "4      Mayon       2018 Jan 13-Mar 18   \n",
      "..       ...                      ...   \n",
      "217    Mayon           1616 Feb 19-24   \n",
      "218     Taal                1605-1611   \n",
      "219     Taal                     1591   \n",
      "220     Taal                     1572   \n",
      "221    Iraya                     1454   \n",
      "\n",
      "                                              Eruption  \n",
      "0                                             Phreatic  \n",
      "1                                             Phreatic  \n",
      "2                               Phreatomagmatic bursts  \n",
      "3    Hydrovolcanic (phreatic, phreatomagmatic, hydr...  \n",
      "4                                    *Lava fountaining  \n",
      "..                                                 ...  \n",
      "217                                                     \n",
      "218                                                     \n",
      "219                                           Phreatic  \n",
      "220                                    Phreatomagmatic  \n",
      "221                                             Pelean  \n",
      "\n",
      "[222 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "# Suppress the InsecureRequestWarning\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Initialize an empty list to store data dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Loop through all 12 pages\n",
    "for page_number in range(1, 13):\n",
    "    url = f'https://wovodat.phivolcs.dost.gov.ph/volcano/erupt-history?page={page_number}'\n",
    "    page = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Find the table on the page\n",
    "    table = soup.find_all('table')[1]\n",
    "    \n",
    "    # Extract column headers\n",
    "    history_record_titles = table.find_all('th')\n",
    "    history_table_titles = [title.text for title in history_record_titles]\n",
    "    \n",
    "    # Extract data rows\n",
    "    data_rows = table.find_all('tr')[2:]  # Skip the first row as it contains headers\n",
    "    \n",
    "    # Loop through data rows and append them to the data list as dictionaries\n",
    "    for row in data_rows:\n",
    "        row_data = [cell.text for cell in row.find_all('td')]\n",
    "        data_dict = dict(zip(history_table_titles, row_data))\n",
    "        data_list.append(data_dict)\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "final_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save the DataFrame as a CSV file with the specified path\n",
    "final_df.to_csv(r'C:\\Users\\user\\Desktop\\Web Scraping\\Test.csv', index=False)\n",
    "\n",
    "# Display the scraped data\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0dc046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
